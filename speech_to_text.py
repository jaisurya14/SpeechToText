# -*- coding: utf-8 -*-
"""Speech To Text

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HxNYsPGFLj4uSAB2NgfwjV9TRS9fa7pW
"""

from google.colab import drive
drive.mount('/content/drive')

cd drive/

cd My Drive/

cd Speech

ls

!pip install jiwer

pip install ipython

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from IPython import display 
from jiwer import wer

data_url = "https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
data_path =  keras.utils.get_file("LJSpeech-1.1",data_url, untar=True)

wavs_path = data_path + "/wavs"
metadata_path = data_path + "/metadata.csv"

metadata_df = pd.read_csv(metadata_path, sep="|", header=None, quoting=3)

metadata_df.tail()

metadata_df.head(10)

metadata_df.columns = ["file_name", "transcription","normalized_transcription"]
metadata_df = metadata_df[["file_name", "normalized_transcription"]]
metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)
metadata_df.head(3)

split = int(len(metadata_df)*0.90)
df_train = metadata_df[:split]
df_val = metadata_df[split:]
print(f"Size of training set: {len(df_train)}")
print(f"Size of training set: {len(df_val)}")

characters = [x for x in "abcdefghijklmnopqrstuvwxyz'?! "]
char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token="")
num_to_char = keras.layers.StringLookup(vocabulary = char_to_num.get_vocabulary(), oov_token="", invert=True)
print(f"The Vocabulary is:{char_to_num.get_vocabulary()}"
f"size={char_to_num.vocabulary_size()}")

framelength = 256
framestep = 160
fftlength = 384

def encode_single_sample(wav_file, label):
  file = tf.io.read_file(wavs_path + wav_file + ".wav")
  audio = tf.audio.decode_wav(file)
  audio = tf.squeeze(audio, axis=.1)
  audio = tf.cast(audio, tf.float32)
  spectogram = tf.signal.stft(audio, framelength=framelength, framestep=framestep, fftlength=fftlength)
  spectpgram = tf.abs(spectogram)
  spectogram = tf.math.pow(spectogram, 0.5)
  means = tf.math.reduce_mean(spectogram, 1, keepdims = True)
  stddevs = tf.math.reduce_std(spectogram, 1, keepdims = True )
  spectogram = (spectogram - means) / (stddevs + 1e-10)
  label = tf.strings.lower(label)
  label = tf.strings.unicode_split(label, input_encodings="UTF-8")
  label = char_to_num(label)
  return spectogram, label

batch_size = 32
train_dataset = tf.data.Dataset.from_tensor_slices(
    (list(df_train["file_name"]),list(df_train["normalized_transcription"]))
    )

train_dataset = (
    train_dataset.map(encode_single_sample,num_parallel_calls=tf.data.AUTOTUNE)
    .padded_batch(batch_size)
    .prefetch(buffer_size=tf.data.AUTOTONE)
    )


validation_dataset = tf.data.Dataset.from_tensor_slices(
    (list(df_val["file_name"]),list(df_val["normalized_transcription"]))
    )
validation_dataset = (
    validation_dataset.map(encode_single_sample,num_parallel_calls=tf.data.AUTOTUNE)
    .padded_batch(batch_size)
    .prefetch(buffer_size=tf.data.AUTOTUNE)
     )

fig = plt.figure(figsize=(8,5))
for batch in train_dataset.take(1):
  spectogram = batch[0][0].numpy()
  spectogram = np.array([np.trim.zeros(x) for x in np.transpose(spectogram)])
  label = batch[1][0]
  label = tf.strings.reduce_join(num_to_char(label)).numpy().decode("utf-8")
  ax = plt.subplot(2,1,1)
  ax.imshow(spectogram, vmax=1)
  ax.set_title(label)
  ax.axis("off")

  file = tf.io.read_file(wavs_path + list(df_train["file_name"])[0] + ".wav")
  audio = tf.audio.decode_wav(file)
  audio = audio.numpy()
  ax = plt.subplot(2,1,2)
  plx.plot(audio)
  ax.set_title("Signal Wave")
  ax.set_xlim(0, len(audio))
  display.display(display.Audio(np.transpose(audio),rate=16000))
  plt.show()



